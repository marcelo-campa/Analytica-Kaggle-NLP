{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio Natural Language Processing with Disaster Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zf = zipfile.ZipFile('./data/nlp-getting-started.zip')\n",
    "train = pd.read_csv(zf.open('train.csv'))\n",
    "test = pd.read_csv(zf.open('test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int64 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 102.1+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A coluna 'keyword' tem potencial, ja que no treino e no teste tem poucos dados faltantes\n",
    "    - Preencher com label 'faltante' nos faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HAPPENING',\n",
       " 'NOW',\n",
       " '-',\n",
       " 'HATZOLAH',\n",
       " 'EMS',\n",
       " 'AMBULANCE',\n",
       " 'RESPONDING',\n",
       " 'WITH',\n",
       " 'DUAL',\n",
       " 'SIRENS',\n",
       " 'AND\\x89Ã›_',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/SeK6MQ6NJF']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(train['text'][200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_nltk = list(stopwords.words('english'))\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_train = count_vectorizer.fit_transform(train['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr_matrix(count_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Our',\n",
       " 'Deeds',\n",
       " 'are',\n",
       " 'the',\n",
       " 'Reason',\n",
       " 'of',\n",
       " 'this',\n",
       " '#',\n",
       " 'earthquake',\n",
       " 'May',\n",
       " 'ALLAH',\n",
       " 'Forgive',\n",
       " 'us',\n",
       " 'all']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checando a tokenizacao\n",
    "\n",
    "word_tokenize(train['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def tweet_tokenize_column(df, column):\n",
    "    \"\"\" \n",
    "        This function gets the Dataframe and the name of a column (String) containing texts (Strings) and returns\n",
    "        a list of lists containing the tokenized text. It also turns every token to it's lower form.\n",
    "        \n",
    "        Input: Pandas DataFrame, String\n",
    "        Return: Nested List\n",
    "    \"\"\"\n",
    "    \n",
    "    tweet_tokenizer = TweetTokenizer()\n",
    "    \n",
    "    # List of sentences\n",
    "    list_sent = [tweet_tokenizer.tokenize(sent) for sent in df[column].values]\n",
    "    \n",
    "    # List of sentences excluding stopword tokens\n",
    "    list_sent_no_stop = [[token.lower() \n",
    "                           for token in sent \n",
    "                           if token not in stopwords.words('english')] \n",
    "                           for sent in list_sent]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return list_sent_no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sent_train = tweet_tokenize_column(train,'text')\n",
    "tokenized_sent_test = tweet_tokenize_column(test,'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['our', 'deeds', 'reason', '#earthquake', 'may', 'allah', 'forgive', 'us'],\n",
       " ['forest', 'fire', 'near', 'la', 'ronge', 'sask', '.', 'canada']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sent_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['just', 'happened', 'terrible', 'car', 'crash'],\n",
       " ['heard',\n",
       "  '#earthquake',\n",
       "  'different',\n",
       "  'cities',\n",
       "  ',',\n",
       "  'stay',\n",
       "  'safe',\n",
       "  'everyone',\n",
       "  '.']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sent_test[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sent_all = tokenized_sent_train + tokenized_sent_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando o TF-IDF nos datasets. Esses tem como caracteristicas:\n",
    "- Contem palavras somente em letra minuscula\n",
    "- Nao tem stopwords\n",
    "- Foi tokenizado com o TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rafael\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['#hail', '#haildamage']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Funcao auxiliar para bypass do tokenizador, uma vez que este passo ja foi feito.\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "tfidf_all = TfidfVectorizer(tokenizer=identity_tokenizer, stop_words='english', lowercase=False)    \n",
    "tfidf_all_fit = tfidf_all.fit_transform(tokenized_sent_all)\n",
    "\n",
    "tfidf_all.get_feature_names()[1000:1002]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '#',\n",
       " '##book',\n",
       " '##fukushima',\n",
       " '##youtube',\n",
       " '#0215',\n",
       " '#034',\n",
       " '#039',\n",
       " '#05',\n",
       " '#0518',\n",
       " '#06',\n",
       " '#09',\n",
       " '#1-1st',\n",
       " '#1008pla',\n",
       " '#1008planet',\n",
       " '#124',\n",
       " '#12k',\n",
       " '#140',\n",
       " '#16',\n",
       " '#163',\n",
       " '#17',\n",
       " '#171',\n",
       " '#1oak',\n",
       " '#2015',\n",
       " '#20150613',\n",
       " '#21dayfix',\n",
       " '#22days',\n",
       " '#24',\n",
       " '#26',\n",
       " '#263chat',\n",
       " '#2a',\n",
       " '#2fast2furious',\n",
       " '#2minutemix',\n",
       " '#360wisenews',\n",
       " '#365disasters',\n",
       " '#3682',\n",
       " '#37592',\n",
       " '#38745',\n",
       " '#3novices',\n",
       " '#452',\n",
       " '#4552',\n",
       " '#4playthursdays',\n",
       " '#5sosfam',\n",
       " '#5sosquotes',\n",
       " '#615',\n",
       " '#629',\n",
       " '#666',\n",
       " '#7294',\n",
       " '#7news',\n",
       " '#7newsadl',\n",
       " '#8217',\n",
       " '#8392',\n",
       " '#89x',\n",
       " '#8nn',\n",
       " '#911',\n",
       " '#9973',\n",
       " '#999day',\n",
       " '#9newsmornings',\n",
       " '#aapatwork',\n",
       " '#abandoned',\n",
       " '#abbott',\n",
       " '#abc',\n",
       " '#abc7eyewitness',\n",
       " '#abcnews',\n",
       " '#abha',\n",
       " '#ableg',\n",
       " '#abomb',\n",
       " '#abstorm',\n",
       " '#acapella',\n",
       " '#accident',\n",
       " '#accidentalprophecy',\n",
       " '#acenewsdesk',\n",
       " '#achedin',\n",
       " '#acreativedc',\n",
       " '#act',\n",
       " '#action',\n",
       " '#actionmoviestaughtus',\n",
       " '#adani',\n",
       " '#addiction',\n",
       " '#addtexastonext1dtour',\n",
       " '#adiossuperbacterias',\n",
       " '#adjust',\n",
       " '#adult',\n",
       " '#advancedwarfare',\n",
       " '#aeroplane',\n",
       " '#aerospace',\n",
       " '#afc',\n",
       " '#afghanistan',\n",
       " '#afp',\n",
       " '#africa',\n",
       " '#africanbaze',\n",
       " '#africansinsf',\n",
       " '#after',\n",
       " '#afterhaiyan',\n",
       " '#afterlife',\n",
       " '#aftermath',\n",
       " '#aftershock',\n",
       " '#age',\n",
       " '#aids',\n",
       " '#aintsheperty',\n",
       " '#aircraft',\n",
       " '#airplane',\n",
       " '#airport',\n",
       " '#airwaves',\n",
       " '#alameda',\n",
       " '#alaska',\n",
       " '#alaskaseafood',\n",
       " '#albany',\n",
       " '#alberta',\n",
       " '#album',\n",
       " '#aleppo',\n",
       " '#alfa',\n",
       " '#algeria',\n",
       " '#allah',\n",
       " '#alllivesmatter',\n",
       " '#allthekidneybeansandsorbet4misha',\n",
       " '#allthenews',\n",
       " '#alps',\n",
       " '#alrasyid448iturasya',\n",
       " '#alt',\n",
       " '#alwaysaho',\n",
       " '#alwx',\n",
       " '#amageddon',\n",
       " '#amazingracecanada',\n",
       " '#amazon',\n",
       " '#ambulance',\n",
       " '#amerikkka',\n",
       " '#amicospizzato',\n",
       " '#amreading',\n",
       " '#amssummer',\n",
       " '#amsterdam',\n",
       " '#amtrak',\n",
       " '#amwriting',\n",
       " '#an247',\n",
       " '#anarchy',\n",
       " '#anchorage',\n",
       " '#ancient',\n",
       " '#android',\n",
       " '#androidgames',\n",
       " '#animalrescue',\n",
       " '#animals',\n",
       " '#anime',\n",
       " '#annonymous',\n",
       " '#annoucement',\n",
       " '#anonymous',\n",
       " '#anthrax',\n",
       " '#anti-terrorism',\n",
       " '#anticipate',\n",
       " '#antioch',\n",
       " '#anxietyproblems',\n",
       " '#aogashima',\n",
       " '#aoms',\n",
       " '#ap',\n",
       " '#aphid',\n",
       " '#apocalypse',\n",
       " '#apocalyptic',\n",
       " '#apple',\n",
       " '#appreciativeinquiry',\n",
       " \"#appy's\",\n",
       " '#aquarius',\n",
       " '#ar',\n",
       " '#archipelagowolves',\n",
       " '#archives',\n",
       " '#ariz',\n",
       " '#arizona',\n",
       " '#arkansas',\n",
       " '#armageddon',\n",
       " '#around',\n",
       " '#arrestpastornganga',\n",
       " '#arsenal',\n",
       " '#arsonist',\n",
       " '#art',\n",
       " '#artectura',\n",
       " '#article',\n",
       " '#artisteoftheweekfact',\n",
       " '#artistsunited',\n",
       " '#arts',\n",
       " '#artwork',\n",
       " '#arwx',\n",
       " '#asae15',\n",
       " '#asap',\n",
       " '#ashes',\n",
       " '#ashes2015',\n",
       " '#ashes2ashes',\n",
       " '#ashestoashes',\n",
       " '#ashville',\n",
       " '#asia',\n",
       " '#asian',\n",
       " '#askceeps',\n",
       " '#askcharley',\n",
       " '#askconnor',\n",
       " '#askforalaska',\n",
       " '#askh3cz',\n",
       " '#ass',\n",
       " '#assassins',\n",
       " '#assnchat',\n",
       " '#astrology',\n",
       " '#atk',\n",
       " '#atlanta',\n",
       " '#atombomb',\n",
       " '#atomicbomb',\n",
       " '#atx',\n",
       " '#au',\n",
       " '#auction',\n",
       " '#audiobook',\n",
       " '#auditiontime',\n",
       " '#aus',\n",
       " '#auspol',\n",
       " '#australia',\n",
       " '#author',\n",
       " '#autism',\n",
       " '#autismawareness',\n",
       " '#autistic',\n",
       " '#autoaccidents',\n",
       " '#autoinsurance',\n",
       " '#avalanche',\n",
       " '#aviationaddicts',\n",
       " '#avigdorliberman',\n",
       " '#avril',\n",
       " '#awesomejobsiri',\n",
       " '#ayekoradio',\n",
       " '#az',\n",
       " '#azwx',\n",
       " '#b2b',\n",
       " '#b2bagency',\n",
       " '#babies',\n",
       " '#babri',\n",
       " '#baby',\n",
       " '#back',\n",
       " '#backtoback',\n",
       " '#badchoices',\n",
       " '#badcredit',\n",
       " '#badge',\n",
       " '#badgeofhonour',\n",
       " '#badirandeal',\n",
       " '#badkitty',\n",
       " '#badotweet',\n",
       " '#bag',\n",
       " '#bahrain',\n",
       " '#bailout',\n",
       " '#bakeofffriends',\n",
       " '#baking',\n",
       " '#bald',\n",
       " '#baltimore',\n",
       " '#baltistan',\n",
       " '#ban',\n",
       " '#bananalivesmatter',\n",
       " '#bancodeseries',\n",
       " '#bangalore',\n",
       " '#bangladesh',\n",
       " '#bangladeshaffected',\n",
       " '#bangladeshflood',\n",
       " '#banking',\n",
       " '#banksy',\n",
       " '#bannukes',\n",
       " '#banthebomb',\n",
       " '#bantrophyhunting',\n",
       " '#bashir',\n",
       " '#bathandnortheastsomerset',\n",
       " '#battlefield',\n",
       " '#bayarea',\n",
       " '#bayonets',\n",
       " '#bb17',\n",
       " '#bbc',\n",
       " '#bbclive',\n",
       " '#bblf',\n",
       " '#bbloggers',\n",
       " '#bbmeg',\n",
       " '#bbshelli',\n",
       " '#bbsnews',\n",
       " '#bc',\n",
       " '#bc19',\n",
       " '#bcpoli',\n",
       " '#bds',\n",
       " '#bears',\n",
       " '#beastburger',\n",
       " '#beauty',\n",
       " '#becarefulharry',\n",
       " '#beclearoncancer',\n",
       " '#beconfident',\n",
       " '#behindthescenes',\n",
       " '#benediction',\n",
       " '#benews',\n",
       " '#bennycapricon',\n",
       " '#besafe',\n",
       " '#best',\n",
       " '#bestdayeva',\n",
       " '#bestnaijamade',\n",
       " '#bestseller',\n",
       " '#besttalkradio',\n",
       " '#bethlehem',\n",
       " '#beyhive',\n",
       " '#beyondgps',\n",
       " '#beyondmeat',\n",
       " '#beyondthebomb',\n",
       " '#bfc630nz',\n",
       " '#bgc14',\n",
       " '#bhramabull',\n",
       " '#bhusa',\n",
       " '#bible',\n",
       " '#biblestudy',\n",
       " '#bidtime',\n",
       " '#bigbrother',\n",
       " '#bigdata',\n",
       " '#bikecommute',\n",
       " '#billings',\n",
       " '#binladen',\n",
       " '#bioterror',\n",
       " '#bioterrorism',\n",
       " '#birdgang',\n",
       " '#bishopfred',\n",
       " '#bitcoin',\n",
       " '#bitcoing',\n",
       " '#biztip',\n",
       " '#bjp',\n",
       " '#black-haired',\n",
       " '#blackforestgateau',\n",
       " '#blackhat',\n",
       " '#blackinamerica',\n",
       " '#blacklivesmatter',\n",
       " '#blackpool',\n",
       " '#bleedinglove',\n",
       " '#blessed',\n",
       " '#blight',\n",
       " '#blinds',\n",
       " '#blizzard',\n",
       " '#blockchain',\n",
       " '#bloodbound',\n",
       " '#bloodymonday',\n",
       " '#blowjob',\n",
       " '#blowltan',\n",
       " '#blowmandyup',\n",
       " '#blowvape',\n",
       " '#bluebell',\n",
       " '#bluehand',\n",
       " '#bluejays',\n",
       " '#body',\n",
       " '#boing',\n",
       " '#bokoharam',\n",
       " '#bokoharm',\n",
       " '#bomb',\n",
       " '#bombed',\n",
       " '#bombeffects',\n",
       " '#boobs',\n",
       " '#book',\n",
       " '#bookboost',\n",
       " '#bookmobile',\n",
       " '#books',\n",
       " '#boomshunga',\n",
       " '#booradleyvancullen',\n",
       " '#borderlands',\n",
       " '#borderlands2',\n",
       " '#bored',\n",
       " '#borrowers',\n",
       " '#boston',\n",
       " '#bosvsnyy',\n",
       " '#bot',\n",
       " '#boulder',\n",
       " '#bowe',\n",
       " '#boxing',\n",
       " '#boy',\n",
       " '#boycottbears',\n",
       " '#boyxboy',\n",
       " '#braininjury',\n",
       " '#breaking',\n",
       " '#breaking144',\n",
       " '#breaking411',\n",
       " '#breakingnews',\n",
       " '#brics',\n",
       " '#bridgetown',\n",
       " '#brisbane',\n",
       " '#britain',\n",
       " '#british',\n",
       " '#britishbakeoff',\n",
       " '#brixton',\n",
       " '#brochure',\n",
       " '#brokelynati',\n",
       " '#bronwynbishop',\n",
       " '#brooklyn',\n",
       " '#brucewillis',\n",
       " '#bts',\n",
       " '#btsprep',\n",
       " '#buffalo',\n",
       " '#builder',\n",
       " '#buildingmuseum',\n",
       " '#burnaby',\n",
       " '#burnfat',\n",
       " '#bush',\n",
       " '#business',\n",
       " '#butgod',\n",
       " '#buybloodonthedanceflooronitunes',\n",
       " '#byebyeroad',\n",
       " '#byebyesoundcloud',\n",
       " '#bynr',\n",
       " '#c4news',\n",
       " '#ca',\n",
       " '#cadenadeseguidores',\n",
       " '#cadrought',\n",
       " '#cafire',\n",
       " '#caiguoqiang',\n",
       " '#cakes',\n",
       " '#calfires',\n",
       " '#calgary',\n",
       " '#calgaryweather',\n",
       " '#cali',\n",
       " '#california',\n",
       " '#callofduty',\n",
       " '#callofmini',\n",
       " '#calwildfires',\n",
       " '#cambridge',\n",
       " '#camera',\n",
       " '#cameroon',\n",
       " '#camplogistics',\n",
       " '#canada',\n",
       " '#canadiansinger',\n",
       " '#cancer',\n",
       " '#cancers',\n",
       " '#cann',\n",
       " '#cannabis',\n",
       " '#canon',\n",
       " '#canonbringit',\n",
       " '#canontattoo',\n",
       " '#canpoli',\n",
       " '#cantmakeitup',\n",
       " '#cantstoplaughing',\n",
       " '#cantwaittoplayinminneapolis',\n",
       " '#capeann',\n",
       " '#captureyyc',\n",
       " '#caraccidentlawyer',\n",
       " '#cardinals',\n",
       " '#care',\n",
       " '#careerarc',\n",
       " '#carfest',\n",
       " '#carlilescanoelivery',\n",
       " '#carpediem',\n",
       " '#cars',\n",
       " '#cashf',\n",
       " '#casper',\n",
       " '#casrf',\n",
       " '#castle',\n",
       " '#casualties',\n",
       " '#casualty',\n",
       " '#catalinas',\n",
       " '#catastrophe',\n",
       " '#catastrophic',\n",
       " '#catfish',\n",
       " '#catfishmtv',\n",
       " '#cats',\n",
       " '#catsofinstagram',\n",
       " '#cawx',\n",
       " '#cbc',\n",
       " '#cbcto',\n",
       " '#cbs',\n",
       " '#cc',\n",
       " '#ccmusic',\n",
       " '#ccot',\n",
       " '#cdcwhistleblower',\n",
       " '#cdnpoli',\n",
       " '#cecilthelion',\n",
       " '#ceciltownship',\n",
       " '#cedarglade2015',\n",
       " '#cern',\n",
       " '#cfc',\n",
       " '#cfr',\n",
       " '#ch4',\n",
       " '#changefortheworse',\n",
       " '#changetheworld',\n",
       " '#chargedup',\n",
       " '#charity',\n",
       " '#charmed',\n",
       " '#charminar',\n",
       " '#chattanooga',\n",
       " '#chawalchorbjp',\n",
       " '#check',\n",
       " '#cheese',\n",
       " '#chelan',\n",
       " '#chelsea',\n",
       " '#chemical',\n",
       " '#chernobyl',\n",
       " '#chester',\n",
       " '#cheyenne',\n",
       " '#chicago',\n",
       " '#chicagoscanner',\n",
       " '#childhood',\n",
       " '#childhooddefined',\n",
       " '#children',\n",
       " '#childsexabuse',\n",
       " '#chile',\n",
       " '#china',\n",
       " '#chinadotcom',\n",
       " '#chiraq',\n",
       " '#choosegod',\n",
       " '#choppergate',\n",
       " '#christians',\n",
       " '#christianvalues',\n",
       " '#christmas',\n",
       " '#chronicillness',\n",
       " '#cityofcalgary',\n",
       " '#cityofmemphis',\n",
       " '#civilian',\n",
       " '#civilwar',\n",
       " '#clambake',\n",
       " '#class_sick',\n",
       " '#claudiomeloni',\n",
       " '#cle',\n",
       " '#cleanpowerplan',\n",
       " '#clevel',\n",
       " '#cleveland',\n",
       " '#client',\n",
       " '#climate',\n",
       " '#climatechange',\n",
       " '#clinton',\n",
       " '#clip',\n",
       " '#cliptv',\n",
       " '#cloud',\n",
       " '#clubbanger',\n",
       " '#cnbc',\n",
       " '#cndpoli',\n",
       " '#cnn',\n",
       " '#cnnhotd',\n",
       " '#cnv',\n",
       " '#co',\n",
       " '#coast2coastdjs',\n",
       " '#cochrane',\n",
       " '#cogxbox',\n",
       " '#coke',\n",
       " '#cold',\n",
       " '#collapse',\n",
       " '#collegeradi',\n",
       " '#colorado',\n",
       " '#coloradoavalanche',\n",
       " '#coloradoavs',\n",
       " '#colts',\n",
       " '#columbus',\n",
       " '#comdev',\n",
       " '#comedy',\n",
       " '#comingsoon',\n",
       " '#commercial',\n",
       " '#companionship',\n",
       " '#compassion',\n",
       " '#computer',\n",
       " '#computers',\n",
       " '#concert',\n",
       " '#concertphotography',\n",
       " '#confused',\n",
       " '#congi',\n",
       " '#conquer',\n",
       " '#consumers',\n",
       " '#content',\n",
       " '#contentmarketing',\n",
       " '#conversations',\n",
       " '#cool',\n",
       " '#copalibertadores',\n",
       " '#copolitics',\n",
       " '#copped',\n",
       " '#coppednews',\n",
       " '#cossack',\n",
       " '#counciling',\n",
       " '#countynews',\n",
       " '#course',\n",
       " '#courts',\n",
       " '#cowboys',\n",
       " '#cowx',\n",
       " '#coya',\n",
       " '#coyi',\n",
       " '#cpu',\n",
       " '#crash',\n",
       " '#crazyideascollege',\n",
       " '#crazyweather',\n",
       " '#crewlist',\n",
       " '#cri',\n",
       " '#cricket',\n",
       " '#crimes',\n",
       " '#criticalmedia',\n",
       " '#crossfit',\n",
       " '#crush',\n",
       " '#crushed',\n",
       " '#cs',\n",
       " '#csismica',\n",
       " '#cta',\n",
       " '#ctot',\n",
       " '#cts',\n",
       " '#cubs',\n",
       " '#cubstalk',\n",
       " '#cuff',\n",
       " '#cufi',\n",
       " '#cum',\n",
       " '#cumshot',\n",
       " '#curse',\n",
       " '#curtainpanel',\n",
       " '#curtains',\n",
       " '#cutekitten',\n",
       " '#cyber',\n",
       " '#cyclone',\n",
       " '#cyprus',\n",
       " '#dabs',\n",
       " '#daesh',\n",
       " '#dagens',\n",
       " '#dam',\n",
       " '#damballa',\n",
       " '#dance',\n",
       " '#danger',\n",
       " '#data',\n",
       " '#date',\n",
       " '#dating',\n",
       " '#datingtips',\n",
       " '#dc',\n",
       " '#dctography',\n",
       " '#dcubecrafts',\n",
       " '#de',\n",
       " '#dead',\n",
       " '#deadgrassandflowers',\n",
       " '#deai',\n",
       " '#deals_uk',\n",
       " '#death',\n",
       " '#deaths',\n",
       " '#debatequestionswewanttohear',\n",
       " '#decisionsondecisions',\n",
       " '#decor',\n",
       " '#deep',\n",
       " '#deepthoughts',\n",
       " '#defendant',\n",
       " '#defundpp',\n",
       " '#deltachildren',\n",
       " '#deluge',\n",
       " '#demolished',\n",
       " '#demolition',\n",
       " '#demonization',\n",
       " '#denver',\n",
       " '#dependency',\n",
       " '#derailed',\n",
       " '#derailingdistractions',\n",
       " '#designgeeks',\n",
       " '#desolation',\n",
       " '#desolationofsmaug',\n",
       " '#destinationimpact',\n",
       " '#destroy',\n",
       " '#destruction',\n",
       " '#detroit',\n",
       " '#deutsche',\n",
       " '#dfir',\n",
       " '#dhhj',\n",
       " '#diabetes',\n",
       " '#diablo',\n",
       " '#diet',\n",
       " '#difficultpeople',\n",
       " '#digitalhealth',\n",
       " '#dime_miloko',\n",
       " '#directioners',\n",
       " '#disabledveterans',\n",
       " '#disaster',\n",
       " '#disasterrecovery',\n",
       " '#disney',\n",
       " '#displaced',\n",
       " '#disposal',\n",
       " '#diversification',\n",
       " '#diyala',\n",
       " '#dna',\n",
       " '#dnb',\n",
       " '#dnr',\n",
       " '#doczone',\n",
       " '#dogbite',\n",
       " '#dogs',\n",
       " '#dogsarebetterthancats',\n",
       " '#doinghashtagsright',\n",
       " '#dolakha',\n",
       " '#dominion',\n",
       " \"#don'tpanic\",\n",
       " '#donbas',\n",
       " '#dontexpectnothing',\n",
       " '#donthate',\n",
       " '#donzilla',\n",
       " '#doomsday',\n",
       " '#doping',\n",
       " '#dorset',\n",
       " '#doublecups',\n",
       " '#doubleghats',\n",
       " '#doubleshot',\n",
       " '#download',\n",
       " '#drained',\n",
       " '#dream',\n",
       " '#driverless',\n",
       " '#driverlesscars',\n",
       " '#drjustinmazur',\n",
       " '#drone',\n",
       " '#drones',\n",
       " '#drought',\n",
       " '#droughtmonitor',\n",
       " '#drum',\n",
       " '#dsp',\n",
       " '#du19',\n",
       " '#dua',\n",
       " '#dubai',\n",
       " '#dublin',\n",
       " '#dubstep',\n",
       " '#dumle',\n",
       " '#dumuzid',\n",
       " '#dune',\n",
       " '#durant',\n",
       " '#duststorm',\n",
       " '#dutton',\n",
       " '#dvd',\n",
       " '#dw',\n",
       " '#dw_english',\n",
       " '#dynamix',\n",
       " '#dysfunctionalredline',\n",
       " '#earth',\n",
       " '#earthquake',\n",
       " '#earthquakenews',\n",
       " '#earthtwerk',\n",
       " '#easternoregon',\n",
       " '#eatshit',\n",
       " '#ebay',\n",
       " '#ebola',\n",
       " '#ebolaoutbreak',\n",
       " '#ebook',\n",
       " '#ecocide',\n",
       " '#econom',\n",
       " '#edm',\n",
       " '#edmonton',\n",
       " '#education',\n",
       " '#edwing',\n",
       " '#egypt',\n",
       " '#elderly',\n",
       " '#elections',\n",
       " '#electro',\n",
       " '#electrocuted',\n",
       " '#electronicmusic',\n",
       " '#elephantintheroom',\n",
       " '#elxn42',\n",
       " '#emergency',\n",
       " '#emm',\n",
       " '#emmerdale',\n",
       " '#ems',\n",
       " '#emsc',\n",
       " '#emsne',\n",
       " '#endangered',\n",
       " '#endconflict',\n",
       " '#endoccupation',\n",
       " '#endofus',\n",
       " '#energiewende',\n",
       " '#energy',\n",
       " '#engvaus',\n",
       " '#enkelbiljett',\n",
       " '#enolagay',\n",
       " '#entertainment',\n",
       " '#entrepreneur',\n",
       " '#entretenimento',\n",
       " '#environment',\n",
       " '#eonlinechat',\n",
       " '#epao',\n",
       " '#erasureisnotequality',\n",
       " '#ergo',\n",
       " '#es',\n",
       " '#escorts',\n",
       " '#etcpb',\n",
       " '#ethical',\n",
       " '#etsymntt',\n",
       " '#euro',\n",
       " '#europe',\n",
       " '#euroquake',\n",
       " '#evacuate',\n",
       " '#evacuation',\n",
       " '#eventspalmbeach',\n",
       " '#everydaynaija',\n",
       " '#evilempire',\n",
       " '#excitedmuch',\n",
       " '#exec',\n",
       " '#expertwhiner',\n",
       " '#explodingkittens',\n",
       " '#explosion',\n",
       " '#extant',\n",
       " '#extinction',\n",
       " '#ey',\n",
       " '#eye',\n",
       " '#eyefacts',\n",
       " '#eyewitness',\n",
       " '#eyewitnesswv',\n",
       " '#ezidigenocide',\n",
       " '#facebook',\n",
       " '#facilitiesmanagement',\n",
       " '#facts',\n",
       " '#failure',\n",
       " '#families',\n",
       " '#family',\n",
       " '#fanarmyfaceoff',\n",
       " '#fanart',\n",
       " '#fant4stic',\n",
       " '#fantasticfour',\n",
       " '#fantasy',\n",
       " '#faroe',\n",
       " '#faroeislands',\n",
       " '#farrakhan',\n",
       " '#fartanxiety',\n",
       " '#fashion',\n",
       " '#fat',\n",
       " '#fatality',\n",
       " '#fatloss',\n",
       " '#fb100',\n",
       " '#fdny',\n",
       " '#fe',\n",
       " '#fear',\n",
       " '#fedex',\n",
       " '#feelingmanly',\n",
       " '#feelthebern',\n",
       " '#fepow',\n",
       " \"#ferguson's\",\n",
       " '#fettilootch',\n",
       " '#fiat',\n",
       " '#fiction',\n",
       " '#fieldworksmells',\n",
       " '#fierce',\n",
       " '#fifa16',\n",
       " '#figureskate',\n",
       " '#filipino',\n",
       " '#film4',\n",
       " '#fingerrockfire',\n",
       " '#finsup',\n",
       " '#fire',\n",
       " '#firefighter',\n",
       " '#firefighters',\n",
       " '#firefighting',\n",
       " '#fireman',\n",
       " '#firemen',\n",
       " '#firenews',\n",
       " '#fires',\n",
       " '#firetruck',\n",
       " '#firstnations',\n",
       " '#firstnet',\n",
       " '#fitness',\n",
       " '#fixitjesus',\n",
       " '#fiya',\n",
       " '#flames',\n",
       " '#flashflood',\n",
       " '#flattened',\n",
       " '#flavorchargedtea',\n",
       " '#flood',\n",
       " '#flooding',\n",
       " '#floods',\n",
       " '#floored4',\n",
       " '#florida',\n",
       " '#floridians',\n",
       " '#fly',\n",
       " '#fn',\n",
       " '#foamcc',\n",
       " '#foamed',\n",
       " '#follow',\n",
       " '#followback',\n",
       " '#followme',\n",
       " '#followngain',\n",
       " '#food',\n",
       " '#foodscare',\n",
       " '#foot',\n",
       " '#forbes',\n",
       " '#forbesasia',\n",
       " '#forecast',\n",
       " '#forest',\n",
       " '#forever',\n",
       " '#forex',\n",
       " '#forgiveness',\n",
       " '#fortitudevalley',\n",
       " '#fortworth',\n",
       " '#foxdebatequestions',\n",
       " '#foxnews',\n",
       " '#fplboss',\n",
       " '#fr',\n",
       " '#fracking',\n",
       " '#france',\n",
       " '#francisunderwood',\n",
       " '#freaky',\n",
       " '#free',\n",
       " '#freeallfour',\n",
       " '#freeamirnow',\n",
       " '#freebitcoin',\n",
       " '#freedom',\n",
       " '#freekashmir',\n",
       " '#freesikhpoliticalprisnors',\n",
       " '#freespeech',\n",
       " '#freshoutofthebox',\n",
       " '#fresno',\n",
       " '#friendship',\n",
       " '#frofrofro',\n",
       " '#fromthedesk',\n",
       " '#fromthefield',\n",
       " '#frontpage',\n",
       " '#frvrgrateful',\n",
       " '#ftsn',\n",
       " '#ftsnnewsdesk',\n",
       " '#ftw',\n",
       " '#fukushima',\n",
       " '#fun',\n",
       " '#fundraise',\n",
       " '#funds',\n",
       " '#funfact',\n",
       " '#funny',\n",
       " '#funnydadcoach',\n",
       " '#funnynews',\n",
       " '#funtimes',\n",
       " '#furniture',\n",
       " '#fusionfestival',\n",
       " '#future',\n",
       " '#fx',\n",
       " '#g90',\n",
       " '#gabon',\n",
       " '#gadget',\n",
       " '#gadgets',\n",
       " '#gallipoli',\n",
       " '#galvnews',\n",
       " '#gamechanger',\n",
       " '#gamefeed',\n",
       " '#gameinsight',\n",
       " '#gameofthrones',\n",
       " '#gamergate',\n",
       " '#gamescom',\n",
       " '#gaming',\n",
       " '#gander',\n",
       " '#gardens',\n",
       " '#gas',\n",
       " '#gawx',\n",
       " '#gay',\n",
       " '#gays',\n",
       " '#gayuk',\n",
       " '#gaza',\n",
       " '#gbbo',\n",
       " '#gbbo2015',\n",
       " \"#ge's\",\n",
       " '#ge2015',\n",
       " '#gemma',\n",
       " '#generalnews',\n",
       " '#geno',\n",
       " '#genocide',\n",
       " '#georgecole',\n",
       " '#geotech',\n",
       " '#german',\n",
       " '#getin',\n",
       " '#getitbeforeitsgone',\n",
       " '#gfe',\n",
       " \"#gfz's\",\n",
       " '#ggindependencmessage',\n",
       " '#gh',\n",
       " '#ghostprotocol',\n",
       " '#gif',\n",
       " '#gilbert23',\n",
       " '#gilgit',\n",
       " '#gishwhes',\n",
       " '#gisuserpr',\n",
       " '#giveaway',\n",
       " '#givebackkalinwhiteaccount',\n",
       " '#glanders',\n",
       " '#glaucoma',\n",
       " '#glimpses',\n",
       " '#global',\n",
       " '#globalwarming',\n",
       " '#globetrottingwino',\n",
       " '#gloucester',\n",
       " '#gls15',\n",
       " '#gma',\n",
       " '#gms',\n",
       " '#gn',\n",
       " '#gnr',\n",
       " '#goblue',\n",
       " '#god',\n",
       " '#gokitgo',\n",
       " '#gold',\n",
       " '#golf',\n",
       " '#goodbye',\n",
       " '#goodreads',\n",
       " '#gop',\n",
       " '#gopdebate',\n",
       " '#gorlovka',\n",
       " '#gospel',\n",
       " '#got',\n",
       " '#govegan',\n",
       " '#govt',\n",
       " '#gpu',\n",
       " '#gradschoolapps',\n",
       " '#grants',\n",
       " '#gravity',\n",
       " '#gravitychat',\n",
       " '#gravitymovie',\n",
       " '#greatbritishbakeoff',\n",
       " \"#greece's\",\n",
       " '#greedyrich',\n",
       " '#greetingcards',\n",
       " '#grindhouse',\n",
       " '#growingupblack',\n",
       " '#growingupincolorado',\n",
       " '#growingupspoiled',\n",
       " '#grupdates',\n",
       " '#guardian',\n",
       " '#guillermo',\n",
       " '#gujaratriot',\n",
       " '#gunfail',\n",
       " '#gunsense',\n",
       " '#guyana',\n",
       " '#gym',\n",
       " '#gymflow',\n",
       " '#gymtime',\n",
       " '#hagerstown',\n",
       " '#haiku',\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_all.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#       token1 token2 token3\n",
    "# train1\n",
    "# train2\n",
    "# .\n",
    "# .\n",
    "# trainN\n",
    "# test1\n",
    "# test2\n",
    "# .\n",
    "# .\n",
    "# testN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(\"TF-IDF DataFrame dimensions: {}\\n\".format(tfidf_train_fit.toarray().shape))\n",
    "# print(\"TF-IDF Number or Features: {}\\n\".format(len(tfidf_train.get_feature_names())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faz sentido, ja que o numero de colunas do ``tfidf_train_fit`` corresponde ao numero de tokens, e a contagem do ``tfidf_train.get_feature_names()`` tambem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_all_df = pd.DataFrame(tfidf_all_fit.toarray(), columns=tfidf_all.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>#</th>\n",
       "      <th>##book</th>\n",
       "      <th>##fukushima</th>\n",
       "      <th>##youtube</th>\n",
       "      <th>#0215</th>\n",
       "      <th>#034</th>\n",
       "      <th>#039</th>\n",
       "      <th>#05</th>\n",
       "      <th>#0518</th>\n",
       "      <th>...</th>\n",
       "      <th>Ã»Ã³</th>\n",
       "      <th>Ã»Ã³categorically</th>\n",
       "      <th>Ã»Ã³her</th>\n",
       "      <th>Ã»Ã³kaiserjaegers</th>\n",
       "      <th>Ã»Ã³kill</th>\n",
       "      <th>Ã»Ã³kody</th>\n",
       "      <th>Ã»Ã³negligence</th>\n",
       "      <th>Ã»Ã³tech</th>\n",
       "      <th>Ã»Ã³we</th>\n",
       "      <th>Ã»Ã³were</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10876 rows Ã— 29720 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         !    #  ##book  ##fukushima  ##youtube  #0215  #034  #039  #05  \\\n",
       "0      0.0  0.0     0.0          0.0        0.0    0.0   0.0   0.0  0.0   \n",
       "1      0.0  0.0     0.0          0.0        0.0    0.0   0.0   0.0  0.0   \n",
       "2      0.0  0.0     0.0          0.0        0.0    0.0   0.0   0.0  0.0   \n",
       "3      0.0  0.0     0.0          0.0        0.0    0.0   0.0   0.0  0.0   \n",
       "4      0.0  0.0     0.0          0.0        0.0    0.0   0.0   0.0  0.0   \n",
       "...    ...  ...     ...          ...        ...    ...   ...   ...  ...   \n",
       "10871  0.0  0.0     0.0          0.0        0.0    0.0   0.0   0.0  0.0   \n",
       "10872  0.0  0.0     0.0          0.0        0.0    0.0   0.0   0.0  0.0   \n",
       "10873  0.0  0.0     0.0          0.0        0.0    0.0   0.0   0.0  0.0   \n",
       "10874  0.0  0.0     0.0          0.0        0.0    0.0   0.0   0.0  0.0   \n",
       "10875  0.0  0.0     0.0          0.0        0.0    0.0   0.0   0.0  0.0   \n",
       "\n",
       "       #0518  ...   Ã»Ã³  Ã»Ã³categorically  Ã»Ã³her  Ã»Ã³kaiserjaegers  Ã»Ã³kill  \\\n",
       "0        0.0  ...  0.0              0.0    0.0              0.0     0.0   \n",
       "1        0.0  ...  0.0              0.0    0.0              0.0     0.0   \n",
       "2        0.0  ...  0.0              0.0    0.0              0.0     0.0   \n",
       "3        0.0  ...  0.0              0.0    0.0              0.0     0.0   \n",
       "4        0.0  ...  0.0              0.0    0.0              0.0     0.0   \n",
       "...      ...  ...  ...              ...    ...              ...     ...   \n",
       "10871    0.0  ...  0.0              0.0    0.0              0.0     0.0   \n",
       "10872    0.0  ...  0.0              0.0    0.0              0.0     0.0   \n",
       "10873    0.0  ...  0.0              0.0    0.0              0.0     0.0   \n",
       "10874    0.0  ...  0.0              0.0    0.0              0.0     0.0   \n",
       "10875    0.0  ...  0.0              0.0    0.0              0.0     0.0   \n",
       "\n",
       "       Ã»Ã³kody  Ã»Ã³negligence  Ã»Ã³tech  Ã»Ã³we  Ã»Ã³were  \n",
       "0         0.0           0.0     0.0   0.0     0.0  \n",
       "1         0.0           0.0     0.0   0.0     0.0  \n",
       "2         0.0           0.0     0.0   0.0     0.0  \n",
       "3         0.0           0.0     0.0   0.0     0.0  \n",
       "4         0.0           0.0     0.0   0.0     0.0  \n",
       "...       ...           ...     ...   ...     ...  \n",
       "10871     0.0           0.0     0.0   0.0     0.0  \n",
       "10872     0.0           0.0     0.0   0.0     0.0  \n",
       "10873     0.0           0.0     0.0   0.0     0.0  \n",
       "10874     0.0           0.0     0.0   0.0     0.0  \n",
       "10875     0.0           0.0     0.0   0.0     0.0  \n",
       "\n",
       "[10876 rows x 29720 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_df = tfidf_all_df[:len(train)]\n",
    "\n",
    "tfidf_test_df = tfidf_all_df[len(train):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-08b7743227f1>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tfidf_train_df[\"target_column\"] = train['target']\n"
     ]
    }
   ],
   "source": [
    "tfidf_train_df[\"target_column\"] = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "7608    1\n",
       "7609    1\n",
       "7610    1\n",
       "7611    1\n",
       "7612    1\n",
       "Name: target_column, Length: 7613, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train_df['target_column']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mi = mutual_info_classif(tfidf_train_df_int.drop(\"target_column\", axis=1), tfidf_train_df_int[\"target_column\"])\n",
    "# mi = pd.Series(mi)\n",
    "# mi.index = intersect_columns\n",
    "# mi.sort_values(ascending=False, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "X = tfidf_train_df.drop(\"target_column\", axis=1)\n",
    "y = tfidf_train_df[\"target_column\"]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=16)\n",
    "\n",
    "clf = LogisticRegression(random_state=16)\n",
    "\n",
    "scores_logistic = cross_val_score(clf, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7047231974377979"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_logistic.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is 0.8903191908577434\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf.fit(X,y)\n",
    "\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "print('Training accuracy is {}'.format(accuracy_score(y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submissao\n",
    "\n",
    "sample_submission = pd.read_csv(zf.open('sample_submission.csv'))\n",
    "\n",
    "y_sub = clf.predict(tfidf_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = sample_submission.copy()\n",
    "sub['target'] = y_sub\n",
    "sub.set_index('id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10861</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10865</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10868</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       target\n",
       "id           \n",
       "0           1\n",
       "2           0\n",
       "3           1\n",
       "9           0\n",
       "11          1\n",
       "...       ...\n",
       "10861       1\n",
       "10865       1\n",
       "10868       1\n",
       "10874       1\n",
       "10875       1\n",
       "\n",
       "[3263 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"./submissions/sub_01.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecao de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif, chi2\n",
    "\n",
    "chi = chi2(X,y)\n",
    "chi = pd.Series(chi[0])\n",
    "chi.index = X.columns\n",
    "chi.sort_values(ascending=False, inplace=True)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "?                  37.196852\n",
       "california         29.250636\n",
       "suicide            24.139547\n",
       ":                  24.103232\n",
       "killed             23.600085\n",
       "                     ...    \n",
       "Ã»Ã²don                    NaN\n",
       "Ã»Ã³categorically          NaN\n",
       "Ã»Ã³kaiserjaegers          NaN\n",
       "Ã»Ã³kill                   NaN\n",
       "Ã»Ã³were                   NaN\n",
       "Length: 29720, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi.to_csv(\"./data/chi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atts = np.linspace(100,10000,100)\n",
    "# list_scores = []\n",
    "# list_var = []\n",
    "\n",
    "# for att in tqdm(atts):\n",
    "    \n",
    "#     list_scores.append(cross_val_score(clf, X[chi[:int(att)].index], y, cv=3).mean())\n",
    "#     list_var.append(cross_val_score(clf, X[chi[:int(att)].index], y, cv=3).var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# int_atts = [int(att) for att in atts]\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# sns.set()\n",
    "# plt.figure(figsize=(14,7))\n",
    "# sns.lineplot(y=list_scores, x=int_atts)\n",
    "# # plt.axvline(x=int_atts[np.array(list_scores[5:]).argmax()+5], color='r')\n",
    "# # plt.xticks(ticks=np.arange(0.00, 0.25, 0.01))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set()\n",
    "# plt.figure(figsize=(14,7))\n",
    "# sns.lineplot(y=list_var, x=int_atts)\n",
    "# # plt.axvline(x=int_atts[np.array(list_var[5:]).argmin()+5], color='r')\n",
    "# # plt.xticks(ticks=np.arange(0.00, 0.25, 0.01))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atts = np.linspace(100,10000,100)\n",
    "# list_scores_over = []\n",
    "\n",
    "# for att in tqdm(atts):\n",
    "#     clf.fit(X[chi[:int(att)].index],y)\n",
    "#     y_pred = clf.predict(X[chi[:int(att)].index])\n",
    "#     acc = accuracy_score(y, y_pred)\n",
    "    \n",
    "#     list_scores_over.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_atts = [int(att) for att in atts]\n",
    "\n",
    "# sns.set()\n",
    "# plt.figure(figsize=(14,7))\n",
    "# sns.lineplot(y=list_scores_over, x=int_atts)\n",
    "# # plt.axvline(x=int_atts[np.array(list_scores[5:]).argmax()+5], color='r')\n",
    "# # plt.xticks(ticks=np.arange(0.00, 0.25, 0.01))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "atts = np.linspace(100,10000,100)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_scores_tts = []\n",
    "\n",
    "# for att in tqdm(atts):\n",
    "#     clf.fit(X_train[chi[:int(att)].index],y_train)\n",
    "#     y_pred = clf.predict(X_test[chi[:int(att)].index])\n",
    "#     acc = accuracy_score(y_test , y_pred)\n",
    "    \n",
    "#     list_scores_tts.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_atts = [int(att) for att in atts]\n",
    "\n",
    "# sns.set()\n",
    "# plt.figure(figsize=(14,7))\n",
    "# sns.lineplot(y=list_scores_tts, x=int_atts)\n",
    "# # plt.axvline(x=int_atts[np.array(list_var[5:]).argmin()+5], color='r')\n",
    "# # plt.xticks(ticks=np.arange(0.00, 0.25, 0.01))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_atts = [int(att) for att in atts]\n",
    "# int_atts[np.array(list_scores_atts).argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train[chi[:3800].index],y_train)\n",
    "y_pred = clf.predict(X_test[chi[:3800].index])\n",
    "acc = accuracy_score(y_test , y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sub_chi = clf.predict(tfidf_test_df[chi[:3800].index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_chi = sample_submission.copy()\n",
    "sub_chi['target'] = y_sub_chi\n",
    "sub_chi.set_index('id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_chi.to_csv(\"./submissions/sub_chi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is 0.8214051214707814\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf_svc = SVC()\n",
    "clf_svc.fit(X_train[chi[:3800].index],y_train)\n",
    "y_pred = clf_svc.predict(X_test[chi[:3800].index])\n",
    "acc = accuracy_score(y_test , y_pred)\n",
    "\n",
    "print('Training accuracy is {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svc.fit(tfidf_train_df[chi[:3800].index],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sub_svc = clf_svc.predict(tfidf_test_df[chi[:3800].index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_svc = sample_submission.copy()\n",
    "sub_svc['target'] = y_sub_svc\n",
    "sub_svc.set_index('id',inplace=True)\n",
    "\n",
    "sub_svc.to_csv(\"./submissions/sub_svc_overfit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atts = [1000,3000,5000]\n",
    "# list_scores_svc = []\n",
    "\n",
    "# for att in tqdm(atts):\n",
    "#     clf_svc.fit(X_train[chi[:int(att)].index],y_train)\n",
    "#     y_pred = clf_svc.predict(X_test[chi[:int(att)].index])\n",
    "#     acc = accuracy_score(y_test , y_pred)\n",
    "    \n",
    "#     list_scores_svc.append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-db6fd936af29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_lg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# tree = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "\n",
    "# scores_tree = cross_val_score(tree, X, y, cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_tree.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_tree = tfidf_df.loc[:,~(tfidf_df.columns == 'target_column')]\n",
    "# Y_tree = tfidf_df.target_column\n",
    "\n",
    "# accuracy_test = []\n",
    "\n",
    "# kf = KFold(n_splits=10)\n",
    "\n",
    "# X = X_tree.values\n",
    "# y = Y_tree.values\n",
    "\n",
    "# for train_index, test_index in kf.split(X_tree):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "#     tree = DecisionTreeClassifier(criterion=\"entropy\", ccp_alpha=0.001226509672564484)\n",
    "    \n",
    "#     tree.fit(X_train, y_train)\n",
    "#     y_test_pred = tree.predict(X_test)\n",
    "#     accuracy_test.append(metrics.accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "\n",
    "# st.t.interval(0.99, len(accuracy_test) - 1, loc=np.mean(accuracy_test), scale=st.sem(accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-Do\n",
    "- Selecionar variaveis mais importantes (Chi^2 | Informacao Mutua)\n",
    "- Selecionar colunas contendo essas variaveis tanto no treino quanto no teste\n",
    "- Testar selecao de variaveis antes para todos os tokens do treino\n",
    "- Testar outros modelos (SVC, NaiveBayes, RidgeClassifier, ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
